{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MoL-MoE Production Template - Custom Regression Tasks\n",
        "\n",
        "This notebook trains MoL-MoE (Mixture of Experts) models on custom CSV datasets.\n",
        "\n",
        "**Features:**\n",
        "- Auto-detects environment (Jupyter/Colab/RunPod)\n",
        "- Fast dependency installation with `uv`\n",
        "- Works with any CSV file (just configure SMILES and target columns)\n",
        "- Includes device-aware training (no more GPU/CPU mismatch errors!)\n",
        "- Trains both MoE+Net and MoE+XGBoost models\n",
        "\n",
        "**Architecture:**\n",
        "- 12 experts: 4x SMI-TED + 4x SELFIES-TED + 4x MHG-GNN\n",
        "- k=4 experts activated per sample\n",
        "- Suitable for regression tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "### Environment Detection & Path Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SETUP CELL 1: Environment Detection & Path Configuration\n",
        "# ============================================================\n",
        "import os\n",
        "import sys\n",
        "import platform\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "IS_COLAB = 'google.colab' in sys.modules\n",
        "IS_JUPYTER = 'ipykernel' in sys.modules\n",
        "DEVICE_NAME = 'GPU' if os.system('nvidia-smi > /dev/null 2>&1') == 0 else 'CPU'\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ENVIRONMENT DETECTION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Platform: {platform.system()}\")\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "print(f\"Runtime: {'Colab' if IS_COLAB else 'Jupyter' if IS_JUPYTER else 'Unknown'}\")\n",
        "print(f\"Device: {DEVICE_NAME}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define base paths using pathlib for cross-platform compatibility\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "MOL_MOE_ROOT = NOTEBOOK_DIR.parent  # models/mol_moe/\n",
        "EXPERTS_DIR = MOL_MOE_ROOT / \"experts\"\n",
        "MOE_DIR = MOL_MOE_ROOT / \"moe\"\n",
        "MATERIALS_ROOT = MOL_MOE_ROOT.parent.parent  # Up to materials/\n",
        "DATA_DIR = MATERIALS_ROOT  # CSVs can be at materials/ level\n",
        "\n",
        "print(f\"\\nPath Configuration:\")\n",
        "print(f\"  Notebook directory: {NOTEBOOK_DIR}\")\n",
        "print(f\"  MoE root: {MOL_MOE_ROOT}\")\n",
        "print(f\"  Data directory: {DATA_DIR}\")\n",
        "\n",
        "# Verify critical paths exist\n",
        "assert EXPERTS_DIR.exists(), f\"Experts directory not found: {EXPERTS_DIR}\"\n",
        "assert MOE_DIR.exists(), f\"MoE directory not found: {MOE_DIR}\"\n",
        "\n",
        "print(\"\\n‚úì All critical paths verified\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### UV Installation & Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SETUP CELL 2: UV Installation & Verification\n",
        "# ============================================================\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "def check_uv():\n",
        "    \"\"\"Check if uv is installed and install if necessary\"\"\"\n",
        "    uv_path = shutil.which('uv')\n",
        "    if uv_path:\n",
        "        result = subprocess.run(['uv', '--version'], capture_output=True, text=True)\n",
        "        print(f\"‚úì uv found: {result.stdout.strip()}\")\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "if not check_uv():\n",
        "    print(\"Installing uv...\")\n",
        "    subprocess.run([\n",
        "        sys.executable, '-m', 'pip', 'install', '--quiet', 'uv'\n",
        "    ], check=True)\n",
        "    \n",
        "    if check_uv():\n",
        "        print(\"‚úì uv installed successfully\")\n",
        "    else:\n",
        "        raise RuntimeError(\"Failed to install uv\")\n",
        "else:\n",
        "    print(\"uv already installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### System Dependencies (Linux only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SETUP CELL 3: Install System Dependencies (Linux only)\n",
        "# ============================================================\n",
        "import platform\n",
        "\n",
        "if platform.system() == 'Linux':\n",
        "    print(\"Installing system dependencies for RDKit...\")\n",
        "    try:\n",
        "        subprocess.run(['apt-get', 'update', '-qq'], capture_output=True, text=True)\n",
        "        subprocess.run([\n",
        "            'apt-get', 'install', '-y', '-qq',\n",
        "            'libxrender1', 'libxext6', 'libsm6', 'libfontconfig1'\n",
        "        ], check=True, capture_output=True, text=True)\n",
        "        print(\"‚úì System dependencies installed\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(\"‚ö†Ô∏è  Warning: Could not install system packages (may need sudo)\")\n",
        "        print(\"   RDKit rendering may not work, but training will still function\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ö†Ô∏è  Warning: apt-get not found (not a Debian/Ubuntu system)\")\n",
        "else:\n",
        "    print(f\"Platform: {platform.system()} - skipping Linux system dependencies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install Dependencies with UV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SETUP CELL 4: Install Dependencies with UV\n",
        "# ============================================================\n",
        "\n",
        "print(\"Installing dependencies with uv (first run: ~3-5 min, cached: ~30 sec)...\")\n",
        "print(\"Configuration:\")\n",
        "print(\"  - Python: 3.10+\")\n",
        "print(\"  - PyTorch: 2.2.0 with CUDA 11.8\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    # Step 1: Install PyTorch with CUDA 11.8\n",
        "    print(\"[1/3] Installing PyTorch with CUDA 11.8...\")\n",
        "    subprocess.run([\n",
        "        'uv', 'pip', 'install',\n",
        "        '--python', sys.executable,\n",
        "        '--index-url', 'https://download.pytorch.org/whl/cu118',\n",
        "        'torch==2.2.0',\n",
        "        'torchvision==0.17.0', \n",
        "        'torchaudio==2.2.0'\n",
        "    ], check=True, capture_output=True)\n",
        "    print(\"      ‚úì PyTorch 2.2.0 with CUDA 11.8 installed\")\n",
        "    \n",
        "    # Step 2: Install torch-scatter\n",
        "    print(\"[2/3] Installing torch-scatter...\")\n",
        "    subprocess.run([\n",
        "        'uv', 'pip', 'install',\n",
        "        '--python', sys.executable,\n",
        "        '--find-links', 'https://data.pyg.org/whl/torch-2.2.0+cu118.html',\n",
        "        'torch-scatter'\n",
        "    ], check=True, capture_output=True)\n",
        "    print(\"      ‚úì torch-scatter installed\")\n",
        "    \n",
        "    # Step 3: Install remaining dependencies\n",
        "    print(\"[3/3] Installing remaining dependencies...\")\n",
        "    remaining_deps = [\n",
        "        'torch-geometric>=2.3.1',\n",
        "        'matplotlib==3.9.2',\n",
        "        'numpy==1.26.4',  # Pin to 1.26.4 to avoid numpy 2.x breaking changes\n",
        "        'pandas>=1.5.3',\n",
        "        'scikit-learn>=1.5.0',\n",
        "        'rdkit>=2024.3.5',\n",
        "        'datasets>=2.13.1',\n",
        "        'huggingface-hub',\n",
        "        'transformers==4.44.2',  # Pinned version compatible with BART models\n",
        "        'selfies>=2.1.0',\n",
        "        'tqdm>=4.66.4',\n",
        "        'xgboost==2.1.3',  # Updated version compatible with numpy 1.26.x\n",
        "        'seaborn',\n",
        "    ]\n",
        "    \n",
        "    subprocess.run([\n",
        "        'uv', 'pip', 'install',\n",
        "        '--python', sys.executable,\n",
        "    ] + remaining_deps, check=True, capture_output=True)\n",
        "    print(\"      ‚úì All dependencies installed successfully\")\n",
        "    \n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"\\n‚ùå Installation failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# Verify installations\n",
        "import torch\n",
        "print(f\"\\nVerification:\")\n",
        "print(f\"  PyTorch version: {torch.__version__}\")\n",
        "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    \n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úì Environment setup complete!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure Module Import Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SETUP CELL 5: Configure Module Import Paths\n",
        "# ============================================================\n",
        "\n",
        "# Add project directories to Python path\n",
        "sys.path.insert(0, str(MOL_MOE_ROOT))\n",
        "sys.path.insert(0, str(EXPERTS_DIR))\n",
        "sys.path.insert(0, str(MOE_DIR))\n",
        "\n",
        "print(\"Module search paths configured:\")\n",
        "for i, path in enumerate(sys.path[:6]):\n",
        "    print(f\"  {i}: {path}\")\n",
        "\n",
        "# Verify imports work\n",
        "try:\n",
        "    from moe import MoE\n",
        "    print(\"\\n‚úì MoE module importable\")\n",
        "except ImportError as e:\n",
        "    print(f\"\\n‚úó MoE import failed: {e}\")\n",
        "    \n",
        "try:\n",
        "    from models import Net\n",
        "    print(\"‚úì Net model importable\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚úó Net import failed: {e}\")\n",
        "\n",
        "print(\"\\n‚úì Module paths configured successfully\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SETUP COMPLETE - Ready to proceed with training!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports & Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Deep learning\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from moe import MoE, train\n",
        "from models import Net\n",
        "\n",
        "# Machine learning\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Chemistry\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors\n",
        "\n",
        "# Try to enable PandasTools rendering\n",
        "try:\n",
        "    from rdkit.Chem import PandasTools\n",
        "    PandasTools.RenderImagesInAllDataFrames(True)\n",
        "    print(\"‚úì RDKit rendering enabled\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  RDKit rendering disabled (missing system libraries)\")\n",
        "    print(\"   Training will work normally\")\n",
        "\n",
        "def normalize_smiles(smi, canonical=True, isomeric=False):\n",
        "    try:\n",
        "        normalized = Chem.MolToSmiles(\n",
        "            Chem.MolFromSmiles(smi), canonical=canonical, isomericSmiles=isomeric\n",
        "        )\n",
        "    except:\n",
        "        normalized = None\n",
        "    return normalized\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Configuration\n",
        "\n",
        "**‚ö†Ô∏è EDIT THIS SECTION** - Configure your custom CSV file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# USER CONFIGURATION - EDIT THIS SECTION\n",
        "# ============================================\n",
        "\n",
        "# Path to your CSV file\n",
        "DATA_FILE = DATA_DIR / 'train_Caco2_Permeability_Papp_AB.csv'\n",
        "\n",
        "# Column names in your CSV\n",
        "SMILES_COLUMN = 'SMILES'\n",
        "TARGET_COLUMN = 'Caco-2 Permeability Papp A>B'\n",
        "\n",
        "# Task configuration\n",
        "TASK_TYPE = 'regression'  # or 'classification'\n",
        "MODEL_NAME = 'Caco2_Papp_AB'  # Used for checkpoint naming\n",
        "\n",
        "# ============================================\n",
        "\n",
        "# Verify data file exists\n",
        "assert DATA_FILE.exists(), f\"Data file not found: {DATA_FILE}\"\n",
        "\n",
        "print(f\"Selected configuration:\")\n",
        "print(f\"  Data file: {DATA_FILE}\")\n",
        "print(f\"  SMILES column: {SMILES_COLUMN}\")\n",
        "print(f\"  Target column: {TARGET_COLUMN}\")\n",
        "print(f\"  Task type: {TASK_TYPE}\")\n",
        "print(f\"  Model name: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hyperparameters\n",
        "\n",
        "**‚ö†Ô∏è EDIT AS NEEDED** - Adjust training parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# HYPERPARAMETERS - ADJUST AS NEEDED\n",
        "# ============================================\n",
        "\n",
        "# Model architecture\n",
        "input_size = 768          # Embedding dimension (fixed)\n",
        "output_size = 2048        # Output dimension\n",
        "num_experts = 12          # Total experts\n",
        "k = 4                     # Experts activated per sample\n",
        "\n",
        "# Training settings\n",
        "batch_size = 32           # Reduce if OOM (e.g., 16)\n",
        "learning_rate = 1e-4      # Learning rate\n",
        "epochs = 150              # Training epochs\n",
        "dropout = 0.2             # Dropout rate\n",
        "\n",
        "# Output\n",
        "output_dim = 1            # Single target regression\n",
        "\n",
        "# Data split ratios\n",
        "train_ratio = 0.70\n",
        "valid_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# ============================================\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Batch size: {batch_size}\")\n",
        "print(f\"  Learning rate: {learning_rate}\")\n",
        "print(f\"  Epochs: {epochs}\")\n",
        "print(f\"  Experts: {num_experts}, activating {k} per sample\")\n",
        "print(f\"  Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Foundation Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from experts.selfies_ted.load import SELFIES\n",
        "\n",
        "print(\"Loading SELFIES-TED...\")\n",
        "model_selfies = SELFIES()\n",
        "model_selfies.load()\n",
        "print(\"‚úì SELFIES-TED loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from experts.mhg_model.load import load\n",
        "\n",
        "print(\"Loading MHG-GNN...\")\n",
        "mhg_gnn = load()\n",
        "print(\"‚úì MHG-GNN loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from experts.smi_ted_light.load import load_smi_ted, MolTranBertTokenizer\n",
        "\n",
        "print(\"Loading SMI-TED...\")\n",
        "smi_ted = load_smi_ted()\n",
        "print(\"‚úì SMI-TED loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv(DATA_FILE)\n",
        "print(f\"Original dataset shape: {df.shape}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize SMILES\n",
        "print(\"Normalizing SMILES...\")\n",
        "df['canon_smiles'] = df[SMILES_COLUMN].apply(normalize_smiles)\n",
        "\n",
        "# Remove invalid SMILES\n",
        "original_count = len(df)\n",
        "df = df.dropna(subset=['canon_smiles', TARGET_COLUMN])\n",
        "print(f\"Removed {original_count - len(df)} invalid entries\")\n",
        "print(f\"Final dataset shape: {df.shape}\")\n",
        "\n",
        "# Show target statistics\n",
        "print(f\"\\n{TARGET_COLUMN} statistics:\")\n",
        "print(df[TARGET_COLUMN].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize target distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].hist(df[TARGET_COLUMN], bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_xlabel(TARGET_COLUMN)\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title(f'Distribution of {TARGET_COLUMN}')\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "axes[1].boxplot(df[TARGET_COLUMN])\n",
        "axes[1].set_ylabel(TARGET_COLUMN)\n",
        "axes[1].set_title(f'Box Plot of {TARGET_COLUMN}')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "train_df, temp_df = train_test_split(df, test_size=(1-train_ratio), random_state=42)\n",
        "valid_size = valid_ratio / (valid_ratio + test_ratio)\n",
        "valid_df, test_df = train_test_split(temp_df, test_size=(1-valid_size), random_state=42)\n",
        "\n",
        "print(f\"Training set: {len(train_df)} samples\")\n",
        "print(f\"Validation set: {len(valid_df)} samples\")\n",
        "print(f\"Test set: {len(test_df)} samples\")\n",
        "\n",
        "# Prepare data\n",
        "smiles_col = 'canon_smiles'\n",
        "\n",
        "X_train = train_df[smiles_col].to_list()\n",
        "y_train = torch.tensor(train_df[TARGET_COLUMN].values, dtype=torch.float32)\n",
        "\n",
        "X_valid = valid_df[smiles_col].to_list()\n",
        "y_valid = torch.tensor(valid_df[TARGET_COLUMN].values, dtype=torch.float32)\n",
        "\n",
        "X_test = test_df[smiles_col].to_list()\n",
        "y_test = torch.tensor(test_df[TARGET_COLUMN].values, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Pre-Training Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "print(\"Running pre-training validation checks...\\n\")\n",
        "\n",
        "# 1. GPU Check\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"  Memory: {gpu_mem_gb:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU available - training on CPU (slower)\")\n",
        "\n",
        "# 2. Data Check\n",
        "assert DATA_FILE.exists(), f\"‚ùå Data file not found: {DATA_FILE}\"\n",
        "print(f\"‚úì Data file valid: {len(df)} samples\")\n",
        "\n",
        "# 3. Disk Space Check\n",
        "disk_usage = shutil.disk_usage('.')\n",
        "free_gb = disk_usage.free / 1e9\n",
        "assert free_gb > 10, f\"‚ùå Low disk space: {free_gb:.1f} GB\"\n",
        "print(f\"‚úì Disk space: {free_gb:.1f} GB free\")\n",
        "\n",
        "# 4. Create checkpoint directory\n",
        "CHECKPOINT_DIR = NOTEBOOK_DIR / 'checkpoints'\n",
        "CHECKPOINT_DIR.mkdir(exist_ok=True)\n",
        "print(f\"‚úì Checkpoint directory: {CHECKPOINT_DIR}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"All validation checks passed! Ready to train.\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Initialize MoE Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define experts (4 per modality)\n",
        "models = [\n",
        "    smi_ted, smi_ted, smi_ted, smi_ted,                      # SMI-TED\n",
        "    model_selfies, model_selfies, model_selfies, model_selfies,  # SELFIES\n",
        "    mhg_gnn, mhg_gnn, mhg_gnn, mhg_gnn                        # MHG-GNN\n",
        "]\n",
        "\n",
        "# Initialize tokenizer\n",
        "vocab_path = EXPERTS_DIR / 'smi_ted_light' / 'bert_vocab_curated.txt'\n",
        "assert vocab_path.exists(), f\"Vocab file not found: {vocab_path}\"\n",
        "tokenizer = MolTranBertTokenizer(str(vocab_path))\n",
        "\n",
        "# IMPORTANT: Move all expert models to DEVICE BEFORE creating MoE\n",
        "print(\"Moving expert models to device...\")\n",
        "smi_ted.to(DEVICE)\n",
        "model_selfies.to(DEVICE)\n",
        "mhg_gnn.to(DEVICE)\n",
        "print(f\"  SMI-TED device: {smi_ted.device}\")\n",
        "print(f\"  SELFIES device: {model_selfies.device}\")\n",
        "print(f\"  MHG-GNN device: {next(mhg_gnn.model.parameters()).device}\")\n",
        "\n",
        "# Initialize MoE\n",
        "print(\"\\nInitializing MoE model...\")\n",
        "moe_model = MoE(\n",
        "    input_size=input_size,\n",
        "    output_size=output_size,\n",
        "    num_experts=num_experts,\n",
        "    models=models,\n",
        "    tokenizer=tokenizer,\n",
        "    tok_emb=smi_ted.encoder.tok_emb,\n",
        "    k=k,\n",
        "    noisy_gating=True,\n",
        "    verbose=False\n",
        ").to(DEVICE)  # This also sets target device on all experts\n",
        "\n",
        "# Initialize predictor network\n",
        "net = Net(smiles_embed_dim=output_size, dropout=dropout, output_dim=output_dim)\n",
        "net.apply(smi_ted._init_weights)\n",
        "net = net.to(DEVICE)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úì All models initialized and on correct device\")\n",
        "print(\"=\"*50)\n",
        "print(f\"  MoE device: {next(moe_model.parameters()).device}\")\n",
        "print(f\"  Net device: {next(net.parameters()).device}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Loss function and optimizer\n",
        "loss_fn = nn.MSELoss()  # Mean Squared Error for regression\n",
        "\n",
        "params = list(moe_model.parameters()) + list(net.parameters())\n",
        "optim = torch.optim.AdamW(params, lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optim, mode='min', factor=0.5, patience=10, verbose=True\n",
        ")\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    list(zip(X_train, y_train)),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0  # Avoid multiprocessing issues\n",
        ")\n",
        "\n",
        "print(f\"Training batches per epoch: {len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d91565a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Retrieve the module containing the bug\n",
        "# The module is imported as 'moe', not 'moe.moe' based on the project structure\n",
        "moe_module = sys.modules.get(\"moe\")\n",
        "\n",
        "if moe_module and hasattr(moe_module, 'SparseDispatcher'):\n",
        "    print(\"Patching SparseDispatcher.dispatch to fix GPU/CPU device mismatch...\")\n",
        "\n",
        "    # Define the corrected dispatch method\n",
        "    def patched_dispatch(self, inp):\n",
        "        inp = pd.Series(inp)\n",
        "\n",
        "        # --- THE FIX ---\n",
        "        # Ensure indices are moved to CPU/Numpy before passing to pandas iloc\n",
        "        indices = self._batch_index\n",
        "        if isinstance(indices, torch.Tensor):\n",
        "            indices = indices.cpu().numpy()\n",
        "        # ----------------\n",
        "\n",
        "        # Original code continues...\n",
        "        inp_exp = inp.iloc[indices]\n",
        "\n",
        "        # Re-implementing the split logic seen in your traceback\n",
        "        _part_indexes = [sum(self._part_sizes[:i]) for i in range(1, len(self._part_sizes))]\n",
        "        return [list(x) for x in np.split(inp_exp.to_numpy(), _part_indexes, axis=0)]\n",
        "\n",
        "    # Apply the patch to the class\n",
        "    moe_module.SparseDispatcher.dispatch = patched_dispatch\n",
        "    print(\"‚úì Patch applied successfully. You can now run the training loop.\")\n",
        "else:\n",
        "    print(\"‚ùå Could not find 'moe' module or 'SparseDispatcher' class. Make sure you have run the setup cells and 'moe' is imported.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f577205",
      "metadata": {},
      "outputs": [],
      "source": [
        "import inspect\n",
        "import sys\n",
        "import numpy as np\n",
        "import textwrap # Import textwrap for dedenting\n",
        "from experts.selfies_ted.load import SELFIES\n",
        "\n",
        "print(\"Patching SELFIES.encode to fix 'Column' object error...\")\n",
        "\n",
        "# 1. Get the source code of the failing function\n",
        "source = inspect.getsource(SELFIES.encode)\n",
        "\n",
        "# 2. Dedent the source code to remove class-level indentation\n",
        "dedented_source = textwrap.dedent(source)\n",
        "\n",
        "# 3. Define the problematic line and the fix\n",
        "# The error comes from calling .copy() on a datasets Column object\n",
        "buggy_line = 'emb = np.asarray(embedding[\"embedding\"].copy())'\n",
        "# The fix is to cast it to a list first, or pass it directly to asarray (which makes a copy)\n",
        "fixed_line = 'emb = np.asarray(list(embedding[\"embedding\"]))'\n",
        "\n",
        "# 4. Replace the buggy line with the fixed line in the dedented source\n",
        "if buggy_line in dedented_source:\n",
        "    new_source = dedented_source.replace(buggy_line, fixed_line)\n",
        "\n",
        "    # 5. Execute the new function code in the original module's context\n",
        "    # This ensures it has access to necessary imports like 'Dataset' inside the module\n",
        "    module_globals = sys.modules['experts.selfies_ted.load'].__dict__\n",
        "    local_vars = {}\n",
        "    exec(new_source, module_globals, local_vars)\n",
        "\n",
        "    # 6. Replace the method on the class (the function is now in local_vars as 'encode')\n",
        "    SELFIES.encode = local_vars['encode']\n",
        "    print(\"‚úì Patch applied successfully.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Could not locate the exact buggy line. The file might differ from expectations.\")\n",
        "\n",
        "print(\"\\nYou can now re-run the training loop cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom training loop with validation\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    moe_model.train()\n",
        "    net.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for (x, y) in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "        y = y.to(DEVICE)  # CRITICAL FIX: Move y to device\n",
        "        \n",
        "        optim.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        embd, aux_loss = moe_model(x)\n",
        "        y_hat = net(embd).squeeze()\n",
        "        \n",
        "        # Calculate loss\n",
        "        loss = loss_fn(y_hat, y)\n",
        "        total_loss = loss + aux_loss\n",
        "        \n",
        "        # Backward pass\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
        "        optim.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    \n",
        "    # Validation\n",
        "    moe_model.eval()\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_embd, _ = moe_model(X_valid, verbose=False)\n",
        "        valid_preds = net(valid_embd).squeeze()\n",
        "        valid_loss = loss_fn(valid_preds.cpu(), y_valid).item()\n",
        "    \n",
        "    valid_losses.append(valid_loss)\n",
        "    scheduler.step(valid_loss)\n",
        "    \n",
        "    # Save best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        checkpoint_path = CHECKPOINT_DIR / f'best_{MODEL_NAME}_moe_model.pt'\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'moe_state_dict': moe_model.state_dict(),\n",
        "            'net_state_dict': net.state_dict(),\n",
        "            'optimizer_state_dict': optim.state_dict(),\n",
        "            'valid_loss': valid_loss,\n",
        "        }, checkpoint_path)\n",
        "    \n",
        "    # Print progress\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"  Valid Loss: {valid_loss:.4f}\")\n",
        "        print(f\"  Best Valid Loss: {best_valid_loss:.4f}\\n\")\n",
        "\n",
        "print(\"\\n‚úì Training completed!\")\n",
        "print(f\"Best validation loss: {best_valid_loss:.4f}\")\n",
        "print(f\"Model saved to: {CHECKPOINT_DIR / f'best_{MODEL_NAME}_moe_model.pt'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss', alpha=0.8)\n",
        "plt.plot(valid_losses, label='Validation Loss', alpha=0.8)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title(f'Training History - {MODEL_NAME}')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint_path = CHECKPOINT_DIR / f'best_{MODEL_NAME}_moe_model.pt'\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "moe_model.load_state_dict(checkpoint['moe_state_dict'])\n",
        "net.load_state_dict(checkpoint['net_state_dict'])\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
        "print(f\"Checkpoint: {checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "moe_model.eval()\n",
        "net.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_embd, _ = moe_model(X_test, verbose=False)\n",
        "    test_preds = net(test_embd).squeeze()\n",
        "    test_preds_np = test_preds.cpu().numpy()\n",
        "    y_test_np = y_test.numpy()\n",
        "\n",
        "# Calculate metrics\n",
        "rmse = np.sqrt(mean_squared_error(y_test_np, test_preds_np))\n",
        "mae = mean_absolute_error(y_test_np, test_preds_np)\n",
        "r2 = r2_score(y_test_np, test_preds_np)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"TEST SET RESULTS - {MODEL_NAME}\")\n",
        "print(\"=\"*50)\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"MAE:  {mae:.4f}\")\n",
        "print(f\"R¬≤:   {r2:.4f}\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parity plot\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "ax.scatter(y_test_np, test_preds_np, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "min_val = min(y_test_np.min(), test_preds_np.min())\n",
        "max_val = max(y_test_np.max(), test_preds_np.max())\n",
        "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect prediction')\n",
        "\n",
        "ax.set_xlabel(f'Actual {TARGET_COLUMN}', fontsize=12)\n",
        "ax.set_ylabel(f'Predicted {TARGET_COLUMN}', fontsize=12)\n",
        "ax.set_title(f'Parity Plot - {MODEL_NAME}\\nRMSE={rmse:.3f}, R¬≤={r2:.3f}', fontsize=14)\n",
        "ax.legend()\n",
        "ax.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Train XGBoost on MoE Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract embeddings for XGBoost\n",
        "print(\"Extracting embeddings...\")\n",
        "moe_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    xgb_train, _ = moe_model(X_train, verbose=False)\n",
        "    xgb_valid, _ = moe_model(X_valid, verbose=False)\n",
        "    xgb_test, _ = moe_model(X_test, verbose=False)\n",
        "\n",
        "xgb_train = xgb_train.cpu().numpy()\n",
        "xgb_valid = xgb_valid.cpu().numpy()\n",
        "xgb_test = xgb_test.cpu().numpy()\n",
        "\n",
        "y_train_np = y_train.numpy()\n",
        "y_valid_np = y_valid.numpy()\n",
        "\n",
        "print(f\"Train embeddings shape: {xgb_train.shape}\")\n",
        "print(f\"Valid embeddings shape: {xgb_valid.shape}\")\n",
        "print(f\"Test embeddings shape: {xgb_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train XGBoost\n",
        "print(\"Training XGBoost...\")\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=8,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    early_stopping_rounds=50,\n",
        "    eval_metric='rmse'\n",
        ")\n",
        "\n",
        "xgb_model.fit(\n",
        "    xgb_train, y_train_np,\n",
        "    eval_set=[(xgb_valid, y_valid_np)],\n",
        "    verbose=100\n",
        ")\n",
        "\n",
        "print(\"\\n‚úì XGBoost training completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate XGBoost\n",
        "xgb_preds = xgb_model.predict(xgb_test)\n",
        "\n",
        "xgb_rmse = np.sqrt(mean_squared_error(y_test_np, xgb_preds))\n",
        "xgb_mae = mean_absolute_error(y_test_np, xgb_preds)\n",
        "xgb_r2 = r2_score(y_test_np, xgb_preds)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"XGBoost TEST SET RESULTS - {MODEL_NAME}\")\n",
        "print(\"=\"*50)\n",
        "print(f\"RMSE: {xgb_rmse:.4f}\")\n",
        "print(f\"MAE:  {xgb_mae:.4f}\")\n",
        "print(f\"R¬≤:   {xgb_r2:.4f}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Save XGBoost model\n",
        "xgb_model_path = CHECKPOINT_DIR / f'xgboost_{MODEL_NAME}_model.json'\n",
        "xgb_model.save_model(str(xgb_model_path))\n",
        "print(f\"\\n‚úì XGBoost model saved to: {xgb_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare both approaches\n",
        "comparison = pd.DataFrame({\n",
        "    'Model': ['MOE + Net', 'MOE + XGBoost'],\n",
        "    'RMSE': [rmse, xgb_rmse],\n",
        "    'MAE': [mae, xgb_mae],\n",
        "    'R¬≤': [r2, xgb_r2]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"FINAL COMPARISON - {MODEL_NAME}\")\n",
        "print(\"=\"*60)\n",
        "print(comparison.to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "metrics = ['RMSE', 'MAE', 'R¬≤']\n",
        "for idx, metric in enumerate(metrics):\n",
        "    axes[idx].bar(comparison['Model'], comparison[metric],\n",
        "                  color=['steelblue', 'coral'], edgecolor='black', alpha=0.7)\n",
        "    axes[idx].set_ylabel(metric, fontsize=12)\n",
        "    axes[idx].set_title(f'{metric} Comparison', fontsize=12)\n",
        "    axes[idx].grid(alpha=0.3, axis='y')\n",
        "    \n",
        "    for i, v in enumerate(comparison[metric]):\n",
        "        axes[idx].text(i, v, f'{v:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save comparison\n",
        "comparison_path = CHECKPOINT_DIR / f'comparison_{MODEL_NAME}.csv'\n",
        "comparison.to_csv(comparison_path, index=False)\n",
        "print(f\"\\n‚úì Comparison saved to: {comparison_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Make Predictions on New Molecules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Predict on new SMILES\n",
        "new_smiles = [\n",
        "    'CCO',        # Ethanol\n",
        "    'CC(=O)O',    # Acetic acid\n",
        "    'c1ccccc1',   # Benzene\n",
        "]\n",
        "\n",
        "# Normalize\n",
        "new_smiles_canon = [normalize_smiles(s) for s in new_smiles]\n",
        "\n",
        "# Predict using MOE+Net\n",
        "moe_model.eval()\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    new_embd, _ = moe_model(new_smiles_canon, verbose=False)\n",
        "    new_preds = net(new_embd).squeeze().cpu().numpy()\n",
        "\n",
        "# Predict using XGBoost\n",
        "xgb_new_preds = xgb_model.predict(new_embd.cpu().numpy())\n",
        "\n",
        "# Display results\n",
        "results_df = pd.DataFrame({\n",
        "    'SMILES': new_smiles,\n",
        "    'MOE+Net Prediction': new_preds,\n",
        "    'XGBoost Prediction': xgb_new_preds\n",
        "})\n",
        "\n",
        "print(f\"\\nPredictions for {TARGET_COLUMN}:\")\n",
        "print(results_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéâ Training Complete!\n",
        "\n",
        "**What you've accomplished:**\n",
        "- ‚úÖ Trained MoL-MoE model on custom dataset\n",
        "- ‚úÖ Trained XGBoost on MoE embeddings\n",
        "- ‚úÖ Evaluated both models on test set\n",
        "- ‚úÖ Saved checkpoints and comparisons\n",
        "\n",
        "**Next steps:**\n",
        "1. Review model performance metrics\n",
        "2. Try different hyperparameters if needed\n",
        "3. Use trained models for predictions on new data\n",
        "4. Compare with other baseline models\n",
        "\n",
        "**Files saved:**\n",
        "- `checkpoints/best_{MODEL_NAME}_moe_model.pt` - Best MoE model\n",
        "- `checkpoints/xgboost_{MODEL_NAME}_model.json` - XGBoost model\n",
        "- `checkpoints/comparison_{MODEL_NAME}.csv` - Performance comparison"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
